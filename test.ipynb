{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "torch.Size([1, 5, 4, 48, 84])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tp_pt_vae import models\n",
    "\n",
    "n_tracks = 5\n",
    "n_measures = 4\n",
    "measure_resolution = 48\n",
    "n_beats = 4\n",
    "n_pitches = 84\n",
    "d_latent = 64\n",
    "\n",
    "vae = models.VAE(\n",
    "    n_tracks,\n",
    "    n_measures,\n",
    "    measure_resolution,\n",
    "    n_beats,\n",
    "    n_pitches,\n",
    "    d_latent,\n",
    ")\n",
    "\n",
    "_sample = torch.FloatTensor(np.array([np.load(\"/home/ksawada/Documents/lab/lab_research/musegan/data/extracted/data/71843.npy\").astype(np.float32), ]))\n",
    "_sample = torch.permute(_sample, (0, 4, 1, 2, 3))\n",
    "\n",
    "out = vae(_sample)\n",
    "print(out[0][0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 90445/92140 [32:30<00:06, 279.57it/s] Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc61ab120d0>\n",
      " 98%|█████████▊| 90505/92140 [32:30<00:05, 276.89it/s]Traceback (most recent call last):\n",
      "  File \"/home/ksawada/Documents/lab/lab_research/tp_pt_vae/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "  File \"/home/ksawada/Documents/lab/lab_research/tp_pt_vae/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/home/ksawada/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError: can only test a child process\n",
      "100%|██████████| 92140/92140 [33:03<00:00, 46.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones/total = 7289648693 / 7430169600 = 0.9810877928008535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from tp_pt_vae.datasets import PianorollDataset\n",
    "from tp_pt_vae.bin.train import Collater\n",
    "\n",
    "n_tracks = 5\n",
    "n_measures = 4\n",
    "measure_resolution = 48\n",
    "n_beats = 4\n",
    "n_pitches = 84\n",
    "d_latent = 64\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "pin_memory = True\n",
    "\n",
    "dataset = PianorollDataset(\n",
    "    pianoroll_list=\"/home/ksawada/Documents/lab/lab_research/tp_pt_vae/egs/lpd/data/lpd_train.txt\",\n",
    "    n_tracks=n_tracks,\n",
    "    measure_resolution=measure_resolution,\n",
    "    n_pitches=n_pitches,\n",
    "    n_measures=n_measures,\n",
    "    allow_cache=False,\n",
    ")\n",
    "collater = Collater()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collater,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    sampler=None,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "total = len(dataset) * n_tracks * n_measures * measure_resolution * n_pitches\n",
    "ones = 0\n",
    "for i in tqdm(dataloader):\n",
    "    ones += (i == 0).sum().item()\n",
    "\n",
    "print(f\"ones/total = {ones} / {total} = {ones/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018912207199146547"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.9810877928008535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 48, 84, 5)\n",
      "(5, 64, 72)\n",
      "batch: torch.Size([64, 5, 4, 48, 84])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lpd = np.load(\"/home/ksawada/Documents/lab/lab_research/musegan/data/extracted/data/8938.npy\")\n",
    "print(lpd.shape)\n",
    "lod_small = np.load(\"/home/ksawada/Documents/lab/lab_research/musegan-pytorch/data/all/00004109_TRJKCQE128F932623B/50.npy\")\n",
    "print(lod_small.shape)\n",
    "print(\"batch: torch.Size([64, 5, 4, 48, 84])\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットのプレビュー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 48, 84, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "colormap = np.array([\n",
    "    [1., 0., 0.],\n",
    "    [1., .5, 0.],\n",
    "    [0., 1., 0.],\n",
    "    [0., 0., 1.],\n",
    "    [0., .5, 1.],\n",
    "]).T\n",
    "\n",
    "def pianoroll_to_image(pianoroll, colormap=None, inverted=True,\n",
    "                       boundary_width=1, boundary_color=0, frame=False,\n",
    "                       gamma=1.):\n",
    "    \"\"\"\n",
    "    Convert a batched pianoroll array to an image array.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pianoroll : `np.array`, ndim=5\n",
    "        The pianoroll array. The shape is (n_pianorolls, n_bars, n_timestep,\n",
    "        n_pitches, n_tracks).\n",
    "    boundary_width : int\n",
    "        Linewidth of the boundary lines. Default to 0.\n",
    "    boundary_color : int\n",
    "        Grayscale of the boundary lines. Valid values are 0 (black) to 255\n",
    "        (white). Default to 0.\n",
    "    frame : bool\n",
    "        Whether to use a grid frame. Default to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image : `np.array`, ndim=4\n",
    "        The image array.\n",
    "    \"\"\"\n",
    "    if pianoroll.ndim != 5:\n",
    "        raise ValueError(\"Input pianoroll array must have 5 dimensions.\")\n",
    "\n",
    "    # Flip the pitch axis\n",
    "    pianoroll = np.flip(pianoroll, 3)\n",
    "\n",
    "    # Apply the color\n",
    "    if colormap is not None:\n",
    "        pianoroll = np.matmul(1. - colormap, np.expand_dims(pianoroll, -1))\n",
    "        pianoroll = pianoroll.squeeze(-1).clip(0., 1.)\n",
    "\n",
    "    # Apply gamma correction\n",
    "    if gamma != 1.:\n",
    "        pianoroll = pianoroll ** gamma\n",
    "\n",
    "    # Invert the color\n",
    "    if inverted:\n",
    "        pianoroll = 1. - pianoroll\n",
    "\n",
    "    # Quantize the image (minus a small value to avoid casting 256 to 0)\n",
    "    quantized = (pianoroll * 256 - 1e-5).astype(np.uint8)\n",
    "\n",
    "    # Add the boundary lines\n",
    "    if boundary_width:\n",
    "        quantized = np.pad(\n",
    "            quantized,\n",
    "            ((0, 0), (0, 0), (boundary_width, 0), (boundary_width, 0), (0, 0)),\n",
    "            'constant', constant_values=boundary_color)\n",
    "\n",
    "    # Transpose and reshape to get the image array\n",
    "    if colormap is None:\n",
    "        transposed = np.transpose(quantized, (0, 4, 3, 1, 2))\n",
    "        image = np.reshape(\n",
    "            transposed, (-1, transposed.shape[1] * transposed.shape[2],\n",
    "                         transposed.shape[3] * transposed.shape[4], 1))\n",
    "    else:\n",
    "        transposed = np.transpose(quantized, (0, 3, 1, 2, 4))\n",
    "        image = np.reshape(transposed, (\n",
    "            -1, transposed.shape[1], transposed.shape[2] * transposed.shape[3],\n",
    "            transposed.shape[4]))\n",
    "\n",
    "    # Deal with the frame\n",
    "    if boundary_width:\n",
    "        if frame:\n",
    "            image = np.pad(\n",
    "                image,\n",
    "                ((0, 0), (0, boundary_width), (0, boundary_width), (0, 0)),\n",
    "                'constant', constant_values=boundary_color)\n",
    "        else:\n",
    "            image = image[:, boundary_width:, boundary_width:]\n",
    "\n",
    "    return image\n",
    "\n",
    "def image_grid(image, grid_shape, grid_width=3, grid_color=0, frame=True):\n",
    "    \"\"\"\n",
    "    Convert a batched image array to one merged grid image array.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pianoroll : `np.array`, ndim=4\n",
    "        The pianoroll array. The first axis is the batch axis. The second and\n",
    "        third axes are the time and pitch axes, respectively, of the pianorolls.\n",
    "        The last axis is the track axis.\n",
    "    grid_shape : list or tuple of int\n",
    "        Shape of the image grid (height, width).\n",
    "    grid_width : int\n",
    "        Linewidth of the grid. Default to 0.\n",
    "    grid_color : int\n",
    "        Grayscale of the grid. Valid values are 0 (black) to 255 (white).\n",
    "        Default to 0.\n",
    "    frame : bool\n",
    "        Whether to use a grid frame. Default to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    merged : `np.array`, ndim=3\n",
    "        The merged image grid array.\n",
    "    \"\"\"\n",
    "    if len(grid_shape) != 2:\n",
    "        raise ValueError(\"`grid_shape` must be a list or tuple of two \"\n",
    "                         \"integers.\")\n",
    "    if image.ndim != 4:\n",
    "        raise ValueError(\"Input image array must have 4 dimensions.\")\n",
    "\n",
    "    # Slice the array to get the right number of images\n",
    "    sliced = image[:(grid_shape[0] * grid_shape[1])]\n",
    "\n",
    "    # Add the grid lines\n",
    "    if grid_width:\n",
    "        sliced = np.pad(\n",
    "            sliced, ((0, 0), (grid_width, 0), (grid_width, 0), (0, 0)),\n",
    "            'constant', constant_values=grid_color)\n",
    "\n",
    "    # Reshape to split the first (batch) axis into two axes\n",
    "    reshaped = np.reshape(sliced, ((grid_shape[0], grid_shape[1])\n",
    "                                   + sliced.shape[1:]))\n",
    "\n",
    "    # Transpose and reshape to get the image grid\n",
    "    transposed = np.transpose(reshaped, (0, 2, 1, 3, 4))\n",
    "    grid = np.reshape(\n",
    "        transposed, (grid_shape[0] * transposed.shape[1],\n",
    "                     grid_shape[1] * transposed.shape[3], image.shape[-1]))\n",
    "\n",
    "    # Deal with the frame\n",
    "    if grid_width:\n",
    "        if frame:\n",
    "            grid = np.pad(grid, ((0, grid_width), (0, grid_width), (0, 0)),\n",
    "                          'constant', constant_values=grid_color)\n",
    "        else:\n",
    "            grid = grid[:, grid_width:, grid_width:]\n",
    "\n",
    "    return grid\n",
    "\n",
    "def _array_to_image(array, colormap=None):\n",
    "    \"\"\"Convert an array to an image array and return it.\"\"\"\n",
    "    # if array.ndim == 2:\n",
    "    #     return vector_to_image(array)\n",
    "    return pianoroll_to_image(array, colormap)\n",
    "\n",
    "def _save_colored_images(array, suffix, name):\n",
    "    \"\"\"Save the input image.\"\"\"\n",
    "    if 'hard_thresholding' in name:\n",
    "        array = (array > 0).astype(np.float32)\n",
    "    elif 'bernoulli_sampling' in name:\n",
    "        rand_num = np.random.uniform(size=array.shape)\n",
    "        array = (.5 * (array + 1.) > rand_num)\n",
    "        array = array.astype(np.float32)\n",
    "    images = _array_to_image(array, colormap)\n",
    "    return _save_image_grid(images, suffix, name)\n",
    "\n",
    "def _get_filepath(folder_name, name, suffix, ext):\n",
    "    \"\"\"Return the filename.\"\"\"\n",
    "    if suffix:\n",
    "        return os.path.join(\n",
    "            \"./\", folder_name, name,\n",
    "            '{}_{}.{}'.format(name, str(suffix, 'utf8'), ext))\n",
    "    return os.path.join(\n",
    "        \"./\", folder_name, name,\n",
    "        '{}.{}'.format(name, ext))\n",
    "\n",
    "def _save_image_grid(array, suffix, name):\n",
    "    image = image_grid(array, [1, 1])\n",
    "    filepath = _get_filepath('images', name, suffix, 'png')\n",
    "    imageio.imwrite(filepath, image)\n",
    "    return np.array([0], np.int32)\n",
    "\n",
    "lpd = np.load(\"/home/ksawada/Documents/lab/lab_research/musegan/data/extracted/data/56628.npy\")\n",
    "print(lpd.shape)\n",
    "\n",
    "\n",
    "_save_colored_images(np.array([lpd]), None, \"test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
